{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38352c22-caf0-457b-817e-af0ddcab8f9a",
   "metadata": {},
   "source": [
    "## 1. Answer-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd483e8-7030-417c-8715-e69ac93582cd",
   "metadata": {},
   "source": [
    "### Understanding Pooling and Padding in CNN\n",
    "#### Purpose and Benefits of Pooling in CNN\n",
    "Pooling is a fundamental operation in Convolutional Neural Networks (CNNs) that serves to progressively reduce the spatial dimensions (height and width) of the input feature maps. The primary purposes and benefits of pooling include:\n",
    "\n",
    "Dimensionality Reduction: \n",
    "Pooling reduces the size of the feature maps, leading to a decrease in the number of parameters and computational load in the network. This makes the network more efficient and faster to train.\n",
    "\n",
    "Translation Invariance: \n",
    "Pooling helps to make the network invariant to small translations of the input, ensuring that the position of the features within the image has less impact on the final output. This enhances the network's ability to recognize objects regardless of their position in the input image.\n",
    "\n",
    "Overfitting Reduction:\n",
    "By reducing the dimensionality of the feature maps, pooling acts as a form of regularization. It helps to simplify the model, which can lead to improved generalization on unseen data.\n",
    "\n",
    "#### Difference Between Average Pooling and Max Pooling\n",
    "\n",
    "Average Pooling: \n",
    "Average pooling calculates the average value of the elements within each pooling window. It smooths the feature map by averaging out the values, which can preserve more background information and lead to more generalized features. This type of pooling can be beneficial in tasks where the overall structure of the feature maps is important.\n",
    "\n",
    "Max Pooling: \n",
    "Max pooling selects the maximum value from each pooling window. It highlights the most prominent features within the region, which often leads to better performance in practice because it captures the most significant and high-contrast features. Max pooling is more commonly used as it tends to lead to faster convergence during training and often better performance on tasks such as image classification.\n",
    "\n",
    "#### Concept of Padding in CNN and Its Significance\n",
    "Padding in CNNs refers to the process of adding extra pixels around the borders of the input feature map. The primary purposes of padding include:\n",
    "\n",
    "Preservation of Spatial Dimensions: \n",
    "Padding allows the spatial dimensions of the input to be maintained in the output after applying convolutional layers. This is crucial for building deeper networks where maintaining consistent feature map sizes across layers is important.\n",
    "\n",
    "Edge Information Retention:\n",
    "Without padding, the convolution operation would shrink the feature maps and potentially lose information at the borders of the input image. Padding ensures that edge and corner information is preserved, which can be crucial for tasks where edge details are important.\n",
    "\n",
    "Control Over Output Size:\n",
    "Padding enables control over the output size of the feature maps. By adjusting the padding, one can ensure that the output feature maps have the desired dimensions, which is important for certain network architectures and applications.\n",
    "\n",
    "#### Compare and Contrast Zero-Padding and Valid-Padding in Terms of Their Effects on the Output Feature Map Size\n",
    "Zero-Padding: In zero-padding, extra pixels with a value of zero are added around the border of the input feature map. This type of padding increases the size of the feature map, allowing the convolution operation to cover the edges and corners of the input. The resulting output feature map can maintain the same spatial dimensions as the input, depending on the amount of padding added. The formula to calculate the output size with zero-padding is:\n",
    "\n",
    "Output size\n",
    "=\n",
    "(\n",
    "Input size\n",
    "−\n",
    "Filter size\n",
    "+\n",
    "2\n",
    "×\n",
    "Padding\n",
    "Stride\n",
    ")\n",
    "+\n",
    "1\n",
    "Output size=( \n",
    "Stride\n",
    "Input size−Filter size+2×Padding\n",
    "​\n",
    " )+1\n",
    "Valid-Padding: Valid-padding, also known as no padding, does not add any extra pixels around the input feature map. As a result, the convolution operation is applied only to the valid parts of the input where the filter completely fits within the boundaries. This results in an output feature map that is smaller than the input, as the borders are not considered. The formula to calculate the output size with valid-padding is:\n",
    "\n",
    "Output size\n",
    "=\n",
    "(\n",
    "Input size\n",
    "−\n",
    "Filter size\n",
    "Stride\n",
    ")\n",
    "+\n",
    "1\n",
    "Output size=( \n",
    "Stride\n",
    "Input size−Filter size\n",
    "​\n",
    " )+1\n",
    "Since no padding is added, the output feature map will always be reduced by an amount proportional to the filter size.\n",
    "\n",
    "#### Effects on the Output Feature Map Size\n",
    "Zero-Padding: This padding helps in preserving the original spatial dimensions of the input feature map. For example, if the input size is \n",
    "32\n",
    "×\n",
    "32\n",
    "32×32 with a \n",
    "3\n",
    "×\n",
    "3\n",
    "3×3 filter and padding of 1, the output size remains \n",
    "32\n",
    "×\n",
    "32\n",
    "32×32, ensuring that the convolution operation covers all areas of the input.\n",
    "\n",
    "Valid-Padding: This padding reduces the spatial dimensions of the feature map. For instance, with the same input size of \n",
    "32\n",
    "×\n",
    "32\n",
    "32×32 and a \n",
    "3\n",
    "×\n",
    "3\n",
    "3×3 filter with no padding, the output size would be \n",
    "30\n",
    "×\n",
    "30\n",
    "30×30, as the convolution operation does not cover the borders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c79ec5-c9f2-4630-b01c-cbfc157f1060",
   "metadata": {},
   "source": [
    "## 2.Answer-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd963d7-65ae-4121-a3b6-6ae1e039900b",
   "metadata": {},
   "source": [
    "###  Exploring LeNet\n",
    "#### Overview of LeNet-5 Architecture\n",
    "LeNet-5 is a pioneering convolutional neural network (CNN) developed by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner in 1998. It was designed primarily for handwritten digit recognition, particularly the MNIST dataset. LeNet-5 is considered a foundational model in the field of deep learning, demonstrating the power of convolutional neural networks for image classification tasks.\n",
    "\n",
    "### Key Components of LeNet-5 and Their Purposes\n",
    "#### 1.Input Layer:\n",
    "\n",
    "Purpose: To receive the input image.\n",
    "Details: Typically, the input is a 32x32 grayscale image. For MNIST, which has 28x28 images, zero-padding is applied to fit this size.\n",
    "#### 2.C1 - First Convolutional Layer:\n",
    "\n",
    "Purpose: To extract features from the input image.\n",
    "Details: Uses six 5x5 filters, resulting in six 28x28 feature maps (no padding is used here, so the dimensions reduce).\n",
    "#### 3.S2 - First Subsampling (Pooling) Layer:\n",
    "\n",
    "Purpose: To reduce the spatial dimensions of the feature maps and introduce invariance to small translations.\n",
    "Details: Performs average pooling with a 2x2 filter and a stride of 2, resulting in six 14x14 feature maps.\n",
    "#### 4.C3 - Second Convolutional Layer:\n",
    "\n",
    "Purpose: To extract more complex features.\n",
    "Details: Uses sixteen 5x5 filters with varying connections to the previous layer, resulting in sixteen 10x10 feature maps.\n",
    "#### 5.S4 - Second Subsampling (Pooling) Layer:\n",
    "\n",
    "Purpose: Similar to S2, to further reduce the spatial dimensions.\n",
    "Details: Performs average pooling with a 2x2 filter and a stride of 2, resulting in sixteen 5x5 feature maps.\n",
    "#### 6.C5 - Third Convolutional Layer:\n",
    "\n",
    "Purpose: To further extract complex patterns.\n",
    "Details: Uses 120 5x5 filters fully connected to the previous layer, resulting in 120 feature maps of size 1x1.\n",
    "#### 7.F6 - Fully Connected Layer:\n",
    "\n",
    "Purpose: To combine features extracted by previous layers.\n",
    "Details: Consists of 84 neurons, fully connected to the 120 outputs from the previous layer.\n",
    "#### 8.output Layer:\n",
    "\n",
    "Purpose: To classify the input image into one of the predefined categories.\n",
    "Details: Uses a softmax activation function to output probabilities for each of the 10 classes in the case of digit classification.\n",
    "### Advantages and Limitations of LeNet-5\n",
    "#### Advantages:\n",
    "\n",
    "Pioneering Architecture: Introduced key concepts like convolutional layers and pooling layers, setting the foundation for future CNNs.\n",
    "Effective for Small Images: Works well with datasets like MNIST where images are small and the classification task is relatively simple.\n",
    "Efficiency: Computationally less intensive compared to modern deep learning architectures, making it suitable for earlier hardware.\n",
    "Limitations:\n",
    "\n",
    "Scalability: Not suitable for larger and more complex datasets without significant modifications.\n",
    "Capacity: Limited depth and number of parameters restrict its ability to learn more complex patterns.\n",
    "Fixed Filter Sizes: Uses fixed-size filters and pooling windows, which might not be optimal for all tasks.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca827dfd-93b9-4049-a930-4f25548318d4",
   "metadata": {},
   "source": [
    "### Implementation of LeNet-5 Using PyTorch and Training on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f6b717-b6e3-49b4-afcd-525a478cb33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.avg_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.avg_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Prepare the dataset and dataloaders\n",
    "transform = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = LeNet5()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58dfd4-7507-4519-9a4f-21c0c4b6e12f",
   "metadata": {},
   "source": [
    "Evaluation and Insights\n",
    "Upon training the LeNet-5 model on the MNIST dataset, the model achieves high accuracy on the test set, typically around 99%, demonstrating its effectiveness for this specific task. However, its performance on more complex and larger datasets would likely be inadequate due to its limited depth and feature extraction capabilities.\n",
    "\n",
    "Insights:\n",
    "\n",
    "Historical Significance: LeNet-5 laid the groundwork for modern CNN architectures.\n",
    "Simplicity: Its straightforward design makes it an excellent educational tool for understanding basic CNN principles.\n",
    "Limitations in Modern Context: While effective for simple tasks, it needs enhancement or more sophisticated architectures like AlexNet, VGG, ResNet, etc., for handling complex image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510f847-57e6-4499-8ef8-fa3ae2016fab",
   "metadata": {},
   "source": [
    "## 3.Answer-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17c887-1965-40f1-bc5c-b891487a940c",
   "metadata": {},
   "source": [
    "### Overview of AlexNet Architecture\n",
    "AlexNet is a deep convolutional neural network (CNN) architecture that revolutionized the field of computer vision and deep learning by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet significantly outperformed the existing methods at the time and demonstrated the power of deep learning on large-scale image classification tasks.\n",
    "\n",
    "#### The architecture consists of eight layers with learnable parameters: \n",
    "five convolutional layers and three fully connected layers. It also employs max-pooling and dropout for regularization.\n",
    "\n",
    "Architectural Innovations in AlexNet\n",
    "ReLU Activation Function:\n",
    "\n",
    "Innovation: Introduced the use of the Rectified Linear Unit (ReLU) activation function.\n",
    "Impact: Accelerated the training process by mitigating the vanishing gradient problem and allowing for faster convergence compared to traditional activation functions like sigmoid or tanh.\n",
    "Dropout Regularization:\n",
    "\n",
    "Innovation: Applied dropout to the fully connected layers to prevent overfitting.\n",
    "Impact: Improved the generalization of the model by randomly setting a fraction of input units to zero during training, which helps in preventing co-adaptation of neurons.\n",
    "GPU Utilization:\n",
    "\n",
    "Innovation: Leveraged Graphics Processing Units (GPUs) for training the network.\n",
    "Impact: Enabled the training of deeper networks with a large number of parameters on large datasets like ImageNet within a reasonable time frame.\n",
    "Data Augmentation:\n",
    "\n",
    "Innovation: Employed data augmentation techniques such as image translations, horizontal reflections, and patch extractions.\n",
    "Impact: Enhanced the robustness of the model by artificially increasing the size of the training set and reducing overfitting.\n",
    "Role of Different Layers in AlexNet\n",
    "Convolutional Layers:\n",
    "\n",
    "Role: Extract hierarchical features from the input images. Early layers capture low-level features like edges and textures, while deeper layers capture more complex patterns and high-level abstractions.\n",
    "Mechanism: Apply convolution operations with learnable filters, followed by non-linear activations (ReLU).\n",
    "Pooling Layers:\n",
    "\n",
    "Role: Reduce the spatial dimensions of the feature maps and provide translation invariance. Pooling helps in reducing the computational complexity and the number of parameters.\n",
    "Mechanism: Use max-pooling operations to down-sample the feature maps, typically with a 2x2 window and a stride of 2.\n",
    "Fully Connected Layers:\n",
    "\n",
    "Role: Perform high-level reasoning and classification based on the features extracted by the convolutional layers.\n",
    "Mechanism: Connect every neuron in one layer to every neuron in the next layer, followed by ReLU activations and dropout regularization. The final fully connected layer uses a softmax activation to output class probabilities.\n",
    "Implementation of AlexNet Using PyTorch\n",
    "To demonstrate AlexNet, we'll implement it using PyTorch and evaluate its performance on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da78bb82-cc6d-400b-a38b-d9e7de2f4a51",
   "metadata": {},
   "source": [
    "#### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1ac994-6144-45f0-b68a-fab435e1ae3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c070f-7c01-4222-b7db-d7450a70c80f",
   "metadata": {},
   "source": [
    "### Step 2: Define the AlexNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fe67e3-038c-4c97-adb7-4d75b0e8b2cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAlexNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(AlexNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return xclass AlexNet(nn.Module):class AlexNet(nn.Module):class AlexNet(nn.Module):class AlexNet(nn.Module):class AlexNet(nn.Module):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19c55e-dfe1-45df-b917-866d9cd723db",
   "metadata": {},
   "source": [
    "### Step 3: Prepare the Dataset and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b173f02-66d1-4592-b437-62aee5e07ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m      8\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec72f5b-22fb-4845-81e0-d90cbe42f364",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate the Model, Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea54a247-94f6-44d8-8be4-cb4b4cd5142e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AlexNet(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854b81d-d874-4887-a7c1-f1ac9206c254",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23095525-6bd8-4e83-b4b7-01cba845d2eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ccd08-d677-495b-b35c-e4a186fccfab",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7297d3-eb5c-42bc-a414-cb853efcdd58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a696f23-e00f-461c-bc5e-a42ac2783c0e",
   "metadata": {},
   "source": [
    "Evaluation and Insights\n",
    "Upon training and evaluating AlexNet on the CIFAR-10 dataset, you would typically observe a test accuracy in the range of 80-85% depending on hyperparameters and training conditions. This performance demonstrates AlexNet's capability to generalize well on small-scale datasets. However, its true strength is more evident on large-scale datasets like ImageNet.\n",
    "\n",
    "Insights:\n",
    "\n",
    "ReLU and Dropout: The combination of ReLU and dropout proved to be effective in speeding up training and preventing overfitting, respectively.\n",
    "Data Augmentation: Applying data augmentation techniques can significantly enhance model robustness.\n",
    "Computational Demands: Training AlexNet is computationally intensive, highlighting the importance of GPUs in deep learning advancements.\n",
    "Historical Impact: AlexNet set the stage for deeper and more complex models, paving the way for future architectures like VGG, ResNet, and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400ee81-d541-4c30-bc44-1bbcb5d74b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
